# Modified: 2026-02-07T01:00:00Z | Author: COPILOT | Change: Added GPU validation job for local compute proof
name: SLATE CI

on:
  push:
    branches: [main, develop, 'feature/*']
    paths-ignore: ['*.md', 'docs/**', '.github/*.md', 'specs/**']
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

defaults:
  run:
    shell: powershell

jobs:
  lint:
    name: Lint & Format
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: Ruff lint
        run: |
          pip install ruff --quiet 2>$null
          ruff check slate/ agents/ --output-format=github
        continue-on-error: true
      - name: Ruff format check
        run: ruff format --check slate/ agents/ --diff
        continue-on-error: true

  unit-tests:
    name: Unit Tests
    runs-on: [self-hosted, slate]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: Install test deps
        run: pip install pytest pytest-cov pytest-asyncio --quiet 2>$null
        continue-on-error: true
      - name: Run tests
        run: python -m pytest tests/ -v --tb=short --ignore=tests/integration/ -x
        continue-on-error: true

  sdk-validation:
    name: SDK Validation
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: Verify SDK imports
        run: |
          python -c "import slate; print(f'SDK v{slate.__version__}')"
          python -c "import slate.slate_status; print('slate_status OK')"
          python -c "import slate.slate_runtime; print('slate_runtime OK')"
          python -c "import slate.slate_hardware_optimizer; print('hardware_optimizer OK')"
      - name: Version sync
        run: |
          python -c @"
          import tomllib
          with open('pyproject.toml', 'rb') as f:
              c = tomllib.load(f)
          v = c['project']['version']
          import slate
          assert slate.__version__ == v, f'Mismatch: {slate.__version__} != {v}'
          print(f'Version sync OK: {v}')
          "@

  security:
    name: Security Scan
    runs-on: [self-hosted, slate]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: Bandit scan
        run: |
          pip install bandit --quiet 2>$null
          bandit -r slate/ agents/ -ll -ii -x "**/test*"
        continue-on-error: true

  slate-checks:
    name: SLATE Quick Checks
    runs-on: [self-hosted, slate]
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: Validate Tech Tree
        run: |
          python -c @"
          import json
          with open('.slate_tech_tree/tech_tree.json', 'r', encoding='utf-8') as f:
              tree = json.load(f)
          nodes = tree.get('nodes', [])
          completed = sum(1 for n in nodes if n.get('status') == 'complete')
          print(f'Tech Tree: {completed}/{len(nodes)} complete')
          "@
      - name: Validate Task Queue
        run: |
          python -c @"
          import json
          with open('current_tasks.json', 'r', encoding='utf-8') as f:
              data = json.load(f)
          tasks = data.get('tasks', data) if isinstance(data, dict) else data
          if isinstance(tasks, list):
              pending = sum(1 for t in tasks if t.get('status') == 'pending')
              print(f'Task Queue: {pending} pending tasks')
          else:
              print('Task queue OK')
          "@
      - name: Validate pyproject.toml
        run: |
          python -c @"
          import tomllib
          with open('pyproject.toml', 'rb') as f:
              config = tomllib.load(f)
          name = config['project']['name']
          ver = config['project']['version']
          print(f'Project: {name} v{ver}')
          "@

  gpu-validation:
    name: GPU Compute Validation
    runs-on: [self-hosted, slate, gpu]
    timeout-minutes: 10
    env:
      CUDA_VISIBLE_DEVICES: '0,1'
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        run: |
          'E:\11132025\.venv\Scripts' | Out-File -Append $env:GITHUB_PATH
      - name: CUDA Environment Check
        run: |
          nvidia-smi --query-gpu=index,name,driver_version,memory.total --format=csv
      - name: PyTorch CUDA Validation
        run: |
          python -c @"
          import torch
          import time
          import json
          print('=== SLATE GPU Compute Validation ===')
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          assert torch.cuda.is_available(), 'CUDA not available - GPU compute broken!'
          print(f'CUDA version: {torch.version.cuda}')
          print(f'Device count: {torch.cuda.device_count()}')
          results = []
          for i in range(torch.cuda.device_count()):
              dev = torch.device(f'cuda:{i}')
              name = torch.cuda.get_device_name(i)
              cap = torch.cuda.get_device_capability(i)
              mem_gb = torch.cuda.get_device_properties(i).total_memory / 1e9
              print(f'\nGPU {i}: {name} (compute {cap[0]}.{cap[1]}, {mem_gb:.1f} GB)')
              # Warm up
              _ = torch.randn(1024, 1024, device=dev)
              torch.cuda.synchronize(dev)
              # Benchmark: 4096x4096 matrix multiply
              a = torch.randn(4096, 4096, device=dev)
              b = torch.randn(4096, 4096, device=dev)
              torch.cuda.synchronize(dev)
              t0 = time.perf_counter()
              c = torch.matmul(a, b)
              torch.cuda.synchronize(dev)
              elapsed = time.perf_counter() - t0
              tflops = 2 * 4096**3 / elapsed / 1e12
              print(f'  Matmul 4096x4096: {elapsed*1000:.1f}ms ({tflops:.2f} TFLOPS)')
              # Tensor allocation test
              big = torch.zeros(2048, 2048, 32, device=dev)
              alloc_mb = big.element_size() * big.nelement() / 1e6
              print(f'  Allocated {alloc_mb:.0f} MB tensor successfully')
              results.append({'gpu': i, 'name': name, 'tflops': round(tflops, 2), 'ms': round(elapsed*1000, 1)})
              del a, b, c, big
              torch.cuda.empty_cache()
          print(f'\n=== ALL {len(results)} GPUs VALIDATED ===')
          for r in results:
              print(f"  GPU {r['gpu']}: {r['tflops']} TFLOPS ({r['ms']}ms)")
          "@
      - name: Multi-GPU Parallel Compute
        run: |
          python -c @"
          import torch
          import torch.nn as nn
          import time
          if torch.cuda.device_count() < 2:
              print('Single GPU - skipping multi-GPU test')
          else:
              print('=== Multi-GPU Parallel Compute ===')
              # Run matmul on both GPUs simultaneously
              a0 = torch.randn(4096, 4096, device='cuda:0')
              b0 = torch.randn(4096, 4096, device='cuda:0')
              a1 = torch.randn(4096, 4096, device='cuda:1')
              b1 = torch.randn(4096, 4096, device='cuda:1')
              torch.cuda.synchronize()
              t0 = time.perf_counter()
              c0 = torch.matmul(a0, b0)
              c1 = torch.matmul(a1, b1)
              torch.cuda.synchronize()
              elapsed = time.perf_counter() - t0
              combined_tflops = 2 * 2 * 4096**3 / elapsed / 1e12
              print(f'  Dual-GPU parallel matmul: {elapsed*1000:.1f}ms ({combined_tflops:.2f} combined TFLOPS)')
              print(f'  GPU 0 result shape: {c0.shape}, sum: {c0.sum().item():.2f}')
              print(f'  GPU 1 result shape: {c1.shape}, sum: {c1.sum().item():.2f}')
              del a0, b0, c0, a1, b1, c1
              torch.cuda.empty_cache()
              print('=== Multi-GPU PASSED ===')
          "@

  summary:
    name: CI Summary
    runs-on: [self-hosted, slate]
    needs: [lint, unit-tests, sdk-validation, security, slate-checks, gpu-validation]
    if: always()
    steps:
      - name: Generate summary
        run: |
          "## SLATE CI Summary" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Job | Status |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "|-----|--------|" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Lint | ${{ needs.lint.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Unit Tests | ${{ needs.unit-tests.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SDK Validation | ${{ needs.sdk-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| Security | ${{ needs.security.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| SLATE Checks | ${{ needs.slate-checks.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
          "| GPU Validation | ${{ needs.gpu-validation.result }} |" | Out-File -Append $env:GITHUB_STEP_SUMMARY
